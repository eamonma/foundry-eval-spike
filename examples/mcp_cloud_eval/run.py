#!/usr/bin/env python3
"""
MCP Cloud Evaluation Example
============================

Demonstrates fully cloud-based evaluation of agents using MCP tools.
The agent runs in the cloud, MCP tools execute via the MCP server,
and evaluators assess the results - no local execution needed.

Usage:
    python -m mcp_cloud_eval.run
    # or from examples directory:
    python mcp_cloud_eval/run.py
"""

import sys
import time
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from shared import get_clients, print_results, DEFAULT_JUDGE_MODEL
from mcp_cloud_eval.agent import create_mcp_weather_agent
from mcp_cloud_eval.data import TEST_ITEMS


def get_testing_criteria(model: str):
    """Return testing criteria for MCP agent evaluation.

    Data Mapping Semantics (discovered empirically - NOT in official docs):

    - {{item.*}} = INPUT data we provide in test items (documented)
      e.g., {{item.query}}, {{item.tool_definitions}}

    - {{sample.*}} = OUTPUT data generated by running the agent (UNDOCUMENTED)
      e.g., {{sample.output_text}}, {{sample.output_items}}, {{sample.tool_calls}}

    Key insight: Map response to {{sample.output_items}} to include
    the full tool call history, not just {{sample.output_text}} which
    is only the final text response without tool calls.

    NOTE: We searched the official Azure AI Foundry docs and confirmed
    {{sample.*}} variables are not documented anywhere. Only {{item.*}}
    appears in official examples.
    """
    return [
        {
            "type": "azure_ai_evaluator",
            "name": "task_adherence",
            "evaluator_name": "builtin.task_adherence",
            "initialization_parameters": {"deployment_name": model},
            "data_mapping": {
                "query": "{{item.query}}",
                "response": "{{sample.output_items}}",
                "tool_definitions": "{{item.tool_definitions}}",
            },
        },
        {
            "type": "azure_ai_evaluator",
            "name": "tool_call_accuracy",
            "evaluator_name": "builtin.tool_call_accuracy",
            "initialization_parameters": {"deployment_name": model},
            "data_mapping": {
                "query": "{{item.query}}",
                "response": "{{sample.output_items}}",
                "tool_definitions": "{{item.tool_definitions}}",
                "tool_calls": "{{sample.tool_calls}}",
            },
        },
        {
            "type": "azure_ai_evaluator",
            "name": "intent_resolution",
            "evaluator_name": "builtin.intent_resolution",
            "initialization_parameters": {"deployment_name": model},
            "data_mapping": {
                "query": "{{item.query}}",
                "response": "{{sample.output_items}}",
                "tool_definitions": "{{item.tool_definitions}}",
            },
        },
    ]


def create_data_source_config():
    """Return data source config for MCP evaluation."""
    return {
        "type": "custom",
        "item_schema": {
            "type": "object",
            "properties": {
                "query": {"type": "string"},
                "tool_definitions": {"type": "array", "items": {"type": "object"}},
            },
            "required": ["query", "tool_definitions"],
        },
        "include_sample_schema": True,
    }


def create_target_data_source(agent, test_items: list):
    """Create azure_ai_target_completions data source.

    This tells the evaluation system to:
    1. Run the agent with each test query
    2. Let the agent use MCP tools
    3. Capture the full response for evaluation
    """
    return {
        "type": "azure_ai_target_completions",
        "source": {
            "type": "file_content",
            "content": [{"item": item} for item in test_items],
        },
        "input_messages": {
            "type": "template",
            "template": [
                {
                    "type": "message",
                    "role": "user",
                    "content": {
                        "type": "input_text",
                        "text": "{{item.query}}",
                    },
                },
            ],
        },
        "target": {
            "type": "azure_ai_agent",
            "name": agent.name,
            "version": agent.version,
        },
    }


def main():
    """Run MCP cloud evaluation."""
    print("=" * 60)
    print("MCP Cloud Evaluation Example")
    print("=" * 60)

    with get_clients() as (project_client, client):
        # Create agent with MCP tools
        print("\nCreating MCP agent...")
        agent = create_mcp_weather_agent(project_client, DEFAULT_JUDGE_MODEL)
        print(f"  Agent: {agent.name} v{agent.version}")

        # Create evaluation
        print("\nCreating evaluation...")
        eval_obj = client.evals.create(
            name="mcp-cloud-eval-example",
            data_source_config=create_data_source_config(),
            testing_criteria=get_testing_criteria(DEFAULT_JUDGE_MODEL),
        )
        print(f"  Eval: {eval_obj.id}")

        # Run evaluation
        print("\nRunning cloud evaluation...")
        print("  (Agent will be invoked for each test query via MCP)")
        eval_run = client.evals.runs.create(
            eval_id=eval_obj.id,
            name="mcp-cloud-run",
            data_source=create_target_data_source(agent, TEST_ITEMS),
        )
        print(f"  Run: {eval_run.id}")

        # Wait for completion
        while eval_run.status not in ("completed", "failed"):
            eval_run = client.evals.runs.retrieve(
                run_id=eval_run.id, eval_id=eval_obj.id
            )
            print(f"    Status: {eval_run.status}")
            time.sleep(3)

        # Print results
        output_items = list(
            client.evals.runs.output_items.list(
                run_id=eval_run.id, eval_id=eval_obj.id
            )
        )
        print_results(eval_run, output_items)

        print("\nDone!")


if __name__ == "__main__":
    main()
